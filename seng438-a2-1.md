**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#2 – Requirements-Based Test Generation**

| Group \#:      |  1   |
| -------------- | --- |
| Student Names: |   Harris Hasnain   |
|                |   Houssem Zaggar  |
|                |   Spencer van Roessel  |
|                |   Kaylyn Tanton |

# 1 Introduction

This lab served as an introduction to automated unit testing. Beginning with a thorough familiarization of the system under test, JFreeChart, we proceeded to create tests based on Javadoc requirements. Leveraging JUnit, we then translated these requirements to unit tests. To test DataUtilities, we used mocking to simulate its dependency on the Values2D and KeyedValues interfaces. The knowledge we gained in this lab lays a solid foundation that can be scaled up to much larger systems, making it applicable for real-world use in the industry.

# 2 Detailed description of unit test strategy

// including the input partitions you have designed

During this lab, unit tests were developed for two classes, org.jfree.data.Range and org.jfree.data.DataUtilities. In general, most of the tests were developed to the standard of strong robust equivalence class testing with as many multidimensional class partitions as could be conceived. Each combination of equivalent classes was tested and implemented near the partition boundaries, on the partition boundaries, and outside the partition boundaries. Slight variation occurred for different classes and methods that required less or more comprehensive testing due to complexity. 

When testing the range class, the methods selected were contains, getLength, getLowerBound, combine, and intersect. For the contains method, the partitions were made at each of the boundaries of the range to generate the first dimension of equivalence classes. A partition was also made at the test value of zero to divide into a negative and positive equivalent class. For this specific class we did weak equivalence class testing, negative values were tested for some of the return values but not every possible value in the different locations of the ranges tested through robust boundary value testing. One-dimensional equivalence class tests were implemented for getLength and getLowerBound. The classes included the range’s bounds being in the negatives, positives, and split between the two. Boundary value testing was not implemented in this case because the boundary values were more vague and insubstantial. The methods combine and intersect both involved two separate ranges. This required many partitions and an abundant number of single-dimensional equivalent classes. These included different combinations of the two ranges overlapping on the upper bound of the first range, lower bound of the first range, perfectly overlapping, not overlapping, etc. Due to the large number of partitions, boundary value testing was not implemented for these two methods, however, each possible combination of the two ranges was tested for strong equivalence class testing. 

The testing strategy for the methods in the DataUtilities class involved a combination of black-box testing techniques including equivalence classes and boundary value analysis. The equivalence classes were valid input, out-of-bounds input, all-zero values, mixed values, and invalid input, and the boundary classes were the boundaries of those equivalence classes. Each test case was designed to cover specific equivalence / boundary classes, to ensure comprehensive coverage. The boundary value testing was used frequently where applicable, such as testing boundary cases row and column indices.

Mocking was used to simulate the behavior of the Values2D and KeyedValues interfaces, allowing isolated testing of the DataUtilities methods that used those interfaces without dependencies. Mocking makes it much easier to test components / methods that depend on interfaces by allowing the freedom of not having to worry about implementing those interfaces. However, in the context of black-box testing, for mocking to work successfully it must first be tested and confirmed that the interfaces methods work perfectly, before they are mocked for the testing of the function that calls them.

# 3 Test cases developed

For DataUtilities...

For Range...

# 4 How the team work/effort was divided and managed

For this lab teams were divided into groups of two to carry out pair testing. Each duo planned test strategies and wrote unit tests for one of the two classes, Range or DataUtilities. Within the smaller groups, each test was developed closely together in a pair programming fashion. This enabled ideas to be bounced off one another and improved the overall cohesiveness in the organization and style of every unit test in an entire class. Test names, styles, and overall strategies were written consistently for each method evaluated. After the testing of each class was completed, both programming pairs combined and reviewed test suites to produce the final product of the lab. 

# 5 Difficulties encountered, challenges overcome, and lessons learned

The JUnit test design, development and running process was very straightforward due to the concepts being explained in lectures and previous experience using JUnit tests, there were times when it was difficult to determine whether certain test cases should be one test since it is only a single action, or multiple based on the number of assert statements. For example, when creating the 2D number array, it is technically only one action (creating the array), but since the data type is very complex, multiple assert statements were needed to ensure the number array was created as expected. Something that proved to be a challenge was ensuring we tested every possible outcome in the methods when developing our test cases, so that we did not miss any hidden errors. Although the number of methods we had to test were few, covering every possible outcome required a fair amount of unit test design to cover them effectively. From this challenge, a lesson we learned was the importance of making sure that all possible routes through the program are covered during testing, as it can be easier to glance over or miss certain specific paths that could lead to fatal errors for users, and that having a solid process of test design, development and execution will help facilitate this more effectively.

# 6 Comments/feedback on the lab itself

Overall, the content of this lab was very informative in demonstrating the importance of test case development for the purposes of extensive software testing, to make sure that all program outcomes work as expected to provide the ideal experience to the user. It was helpful in teaching us how to implement an effective testing process through design, implementation and execution using JUnit. Relative to last lab, we believe that the contents and structure of this lab handout had a much more efficient layout and explanation, making it easier for us to fully understand our tasks in this lab relative to the previous one.
